{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDyXspY7UC15"
      },
      "source": [
        "# Checkpoint 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_nMmp7KUGJA"
      },
      "source": [
        "**Part A**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOQC2h79TSqZ"
      },
      "source": [
        "https://www.kaggle.com/datasets/omnamahshivai/surgical-dataset-binary-classification\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imSv9XE4egyU"
      },
      "source": [
        "## **Subtask 1**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2TN92QBTa96"
      },
      "source": [
        "Dataset name + link: https://www.kaggle.com/datasets/omnamahshivai/surgical-dataset-binary-classification\n",
        "\n",
        "License/terms: Published by Anesthesia & Analgesia (Wolters Kluwer / International Anesthesia Research Society) and is not open access. All rights are reserved, and reuse, redistribution, or reproduction beyond personal or fair use requires prior permission from the publisher.\n",
        "\n",
        "Prediction task + target definition: Predict whether a surgery will have a complication or not given features, target: `complication`\n",
        "\n",
        "Intended use / decision context: We want to see how certain features, such as biodata on an individual, can be a good predictor on surgeries having complications.\n",
        "\n",
        "Feature dictionary (5-10 key features): https://www.causeweb.org/tshs/datasets/Surgery%20Timing%20Data%20Dictionary.pdf\n",
        "* bmi\n",
        "* baseline_cancer\n",
        "* baseline_cvd\n",
        "* baseline_dementia\n",
        "* baseline_diabetes\n",
        "* age\n",
        "* baseline_osteoart\n",
        "* baseline_digestive\n",
        "* baseline_psych\n",
        "* baseline_pulmonary\n",
        "\n",
        "Known limitations/risks (2-3 bullets):\n",
        "* Because the study is not randomized, it can show association but not causation\n",
        "* Focusing on 30-day mortality may miss longer-term complications or functional outcomes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pDeIWYUlUheH",
        "outputId": "178da08f-2d8a-4722-93c1-f8920a7286f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using Colab cache for faster access to the 'surgical-dataset-binary-classification' dataset.\n",
            "Dataset downloaded to: /kaggle/input/surgical-dataset-binary-classification\n",
            "Files: ['Surgical-deepnet.csv']\n",
            "Dataset shape: (14635, 25)\n",
            "     bmi   Age  asa_status  baseline_cancer  baseline_charlson  baseline_cvd  \\\n",
            "0  19.31  59.2           1                1                  0             0   \n",
            "1  18.73  59.1           0                0                  0             0   \n",
            "2  21.85  59.0           0                0                  0             0   \n",
            "3  18.49  59.0           1                0                  1             0   \n",
            "4  19.70  59.0           1                0                  0             0   \n",
            "\n",
            "   baseline_dementia  baseline_diabetes  baseline_digestive  \\\n",
            "0                  0                  0                   0   \n",
            "1                  0                  0                   0   \n",
            "2                  0                  0                   0   \n",
            "3                  0                  1                   1   \n",
            "4                  0                  0                   0   \n",
            "\n",
            "   baseline_osteoart  ...  complication_rsi  dow  gender   hour  month  \\\n",
            "0                  0  ...             -0.57    3       0   7.63      6   \n",
            "1                  0  ...              0.21    0       0  12.93      0   \n",
            "2                  0  ...              0.00    2       0   7.68      5   \n",
            "3                  0  ...             -0.65    2       1   7.58      4   \n",
            "4                  0  ...              0.00    0       0   7.88     11   \n",
            "\n",
            "   moonphase  mort30  mortality_rsi  race  complication  \n",
            "0          1       0          -0.43     1             0  \n",
            "1          1       0          -0.41     1             0  \n",
            "2          3       0           0.08     1             0  \n",
            "3          3       0          -0.32     1             0  \n",
            "4          0       0           0.00     1             0  \n",
            "\n",
            "[5 rows x 25 columns]\n",
            "Number of duplicate rows: 2902\n",
            "Sample duplicate rows:\n",
            "Shape after removing duplicates: (11733, 25)\n",
            "Missing values per column:\n",
            "bmi                    0\n",
            "Age                    0\n",
            "asa_status             0\n",
            "baseline_cancer        0\n",
            "baseline_charlson      0\n",
            "baseline_cvd           0\n",
            "baseline_dementia      0\n",
            "baseline_diabetes      0\n",
            "baseline_digestive     0\n",
            "baseline_osteoart      0\n",
            "baseline_psych         0\n",
            "baseline_pulmonary     0\n",
            "ahrq_ccs               0\n",
            "ccsComplicationRate    0\n",
            "ccsMort30Rate          0\n",
            "complication_rsi       0\n",
            "dow                    0\n",
            "gender                 0\n",
            "hour                   0\n",
            "month                  0\n",
            "moonphase              0\n",
            "mort30                 0\n",
            "mortality_rsi          0\n",
            "race                   0\n",
            "complication           0\n",
            "dtype: int64\n",
            "\n",
            "Total missing values in dataset: 0\n",
            "\n",
            "Missing percentage per column:\n",
            "bmi                    0.0\n",
            "Age                    0.0\n",
            "asa_status             0.0\n",
            "baseline_cancer        0.0\n",
            "baseline_charlson      0.0\n",
            "baseline_cvd           0.0\n",
            "baseline_dementia      0.0\n",
            "baseline_diabetes      0.0\n",
            "baseline_digestive     0.0\n",
            "baseline_osteoart      0.0\n",
            "baseline_psych         0.0\n",
            "baseline_pulmonary     0.0\n",
            "ahrq_ccs               0.0\n",
            "ccsComplicationRate    0.0\n",
            "ccsMort30Rate          0.0\n",
            "complication_rsi       0.0\n",
            "dow                    0.0\n",
            "gender                 0.0\n",
            "hour                   0.0\n",
            "month                  0.0\n",
            "moonphase              0.0\n",
            "mort30                 0.0\n",
            "mortality_rsi          0.0\n",
            "race                   0.0\n",
            "complication           0.0\n",
            "dtype: float64\n",
            "\n",
            "No missing values found in the dataset.\n",
            "Sum of complication: 3690\n",
            "Sum of mort30: 58\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Download dataset\n",
        "path = kagglehub.dataset_download(\n",
        "    \"omnamahshivai/surgical-dataset-binary-classification\"\n",
        ")\n",
        "\n",
        "print(\"Dataset downloaded to:\", path)\n",
        "\n",
        "# List files in the dataset directory\n",
        "files = os.listdir(path)\n",
        "\n",
        "print(\"Files:\", files)\n",
        "\n",
        "# Load the CSV file (update name if needed)\n",
        "csv_file = \"Surgical-deepnet.csv\"  # this is the main file in the dataset\n",
        "csv_path = os.path.join(path, csv_file)\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# Basic info\n",
        "print(\"Dataset shape:\", df.shape)\n",
        "print(df.head())\n",
        "\n",
        "# Check for duplicate rows\n",
        "num_duplicates = df.duplicated().sum()\n",
        "\n",
        "print(\"Number of duplicate rows:\", num_duplicates)\n",
        "\n",
        "# View duplicates\n",
        "if num_duplicates > 0:\n",
        "    duplicates = df[df.duplicated()]\n",
        "    print(\"Sample duplicate rows:\")\n",
        "\n",
        "# Remove duplicates\n",
        "df_no_duplicates = df.drop_duplicates()\n",
        "print(\"Shape after removing duplicates:\", df_no_duplicates.shape)\n",
        "\n",
        "# Check for missing values in each column\n",
        "missing_per_column = df_no_duplicates.isna().sum()\n",
        "\n",
        "print(\"Missing values per column:\")\n",
        "print(missing_per_column)\n",
        "\n",
        "# Total number of missing values in the dataset\n",
        "total_missing = missing_per_column.sum()\n",
        "print(\"\\nTotal missing values in dataset:\", total_missing)\n",
        "\n",
        "# Percentage of missing values per column\n",
        "missing_percentage = (missing_per_column / len(df_no_duplicates)) * 100\n",
        "\n",
        "print(\"\\nMissing percentage per column:\")\n",
        "print(missing_percentage)\n",
        "\n",
        "# Show only columns that actually have missing values\n",
        "columns_with_missing = missing_per_column[missing_per_column > 0]\n",
        "\n",
        "if not columns_with_missing.empty:\n",
        "    print(\"\\nColumns with missing values:\")\n",
        "    print(columns_with_missing)\n",
        "else:\n",
        "    print(\"\\nNo missing values found in the dataset.\")\n",
        "\n",
        "complication_sum = df_no_duplicates[\"complication\"].sum()\n",
        "mort30_sum = df_no_duplicates[\"mort30\"].sum()\n",
        "\n",
        "print(\"Sum of complication:\", complication_sum)\n",
        "print(\"Sum of mort30:\", mort30_sum)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ART-l024fxgW"
      },
      "source": [
        "## **Subtask 2**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2IpVMeccarot"
      },
      "source": [
        "*Data-Quality Audit*\n",
        "\n",
        "Missingness Summary\n",
        "* The dataset is clean with respect to completeness, as there are 0 missing\n",
        "  values across all 25 columns. Since each feature has a 100% fill rate, so imputation strategies are necessary.\n",
        "\n",
        "\n",
        "Duplicate Rows Check\n",
        "* Significant quantity of duplicates identified: 2902 rows (~19.8% of the original 14,635 entries).\n",
        "* These rows were resultantly removed, leaving a dataset of 11,733 unique rows.\n",
        "\n",
        "Target Distribution\n",
        "* Primary target (`complication`): There are 3690 positive cases out of 11,733. This represents a 31.4% complication rate. This is a relatively healthy distribution for a binary classifier given the size of the dataset.\n",
        "* Secondary Target (`mort30`): There are only 58 positive cases (~0.5%). This is highly imbalanced, and should likely be dropped.\n",
        "\n",
        "Ethical Considerations\n",
        "* The ethical considerations around this data set involve the classification of race. The dataset only uses three different classifications of race- Caucasian, African American, and Other. This is kind of concerning that many Race classifications, if necessary for prediction, are grouped into \"Other\" which we feel like could make the model biased against the other group since there would be more noise surrounding those data points, rather than precisely identifying them uniquely. For that reason we consider dropping the race category."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORS5NycAgrlj"
      },
      "source": [
        "## Subtask 3\n",
        "\n",
        "Possible leakage vectors include mort30 as the mort30 column is an outcome after the surgery happened. Therefore if we are trying to predict if there was a complication in surgery, mort 30 is likely highly indicative of the complication and directly related to it. Additionally, it would not be data that we have access to before surgery, which is ideally when you would want to do this prediction/use this model. To prevent this vector, we drop the mort30 column in order to not include this data in our prediction.\n",
        "\n",
        "Other risks of leakage are related to how the data is preprocessed. In order to prevent these, we should fit any of our preprocessing bits before we split the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lMQ8ZX9_cnjG"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.linear_model import LogisticRegression  # or any model\n",
        "\n",
        "# Drop unwanted columns\n",
        "df_clean = df_no_duplicates.drop(\n",
        "    columns=[\"mort30\", \"moonphase\", \"month\", \"dow\", \"hour\"],\n",
        "    errors=\"ignore\"\n",
        ")\n",
        "\n",
        "# Split features / target\n",
        "X = df_clean.drop(columns=[\"complication\"])\n",
        "y = df_clean[\"complication\"]\n",
        "\n",
        "# 80/20 split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=123, shuffle=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTVp37DtX5Bd"
      },
      "source": [
        "The year isn't included so we don't know the real ordering, which is why we don't perform a time-based split."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3W8LPMmdtTD",
        "outputId": "7837c87c-f14b-4dc6-8b7c-cf41c65e0805"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.749893481039625"
            ]
          },
          "execution_count": 90,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "knn_pipeline = Pipeline(steps=[\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"knn\", KNeighborsClassifier(\n",
        "        n_neighbors=71,\n",
        "        weights=\"distance\",\n",
        "        metric=\"euclidean\"\n",
        "    ))\n",
        "])\n",
        "\n",
        "# Fit ONLY on training data\n",
        "knn_pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Accuracy\n",
        "knn_pipeline.score(X_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpcPGouQeri5",
        "outputId": "60f10441-8e5e-4160-8f31-20290864a092"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[1512  100]\n",
            " [ 487  248]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "y_pred = knn_pipeline.predict(X_test)\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(cm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGpI4uitiPlZ"
      },
      "source": [
        "###Part B\n",
        "We dropped the columns mort30 because it would be a form of data leakage since it is only known after the fact. We dropped moonphase, month, dow, and hour to prevent overfitting. Our Pipeline then involved a Standard scaler, and then Knn classification using the Euclidean metric. We got an accuracy of 74.99% which is not great because the classs split is roughly 70/30 but it does show some promise as other people's models only got around 80%.\n",
        "The confusion matrix is:\n",
        "[[1512  100]\n",
        " [ 487  248]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "56gqVj5DhOV0"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
